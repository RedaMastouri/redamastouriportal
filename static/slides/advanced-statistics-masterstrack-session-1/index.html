<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Statistical Thinking for Machine Learning: Lecture 3</title>
    <meta charset="utf-8" />
    <meta name="author" content="Reda Mastouri UChicago MastersTrack: Coursera Thank you to Gregory Bernstein for parts of these slides" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/robot-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="uchicago_slide.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: left, middle, inverse, title-slide

# Statistical Thinking for Machine Learning: Lecture 3
### Reda Mastouri <br><span style="font-size: 50%;">UChicago MastersTrack: Coursera<br>Thank you to Gregory Bernstein for parts of these slides</span>

---




class: title, title-inv-7, center
count: FALSE

# Recap

--

.center[
.box-1.medium[Understanding data and different data types]
]

--

.center[
.box-1.medium[Distributions, PDF, CDF]
]

--

.center[
.box-1.medium[Sampling and descriptive statistics]
]

--

.center[
.box-1.medium[Hypothesis testing to evaluate a single parameter]
]

--

.center[
.box-1.medium[Bivariate linear model]
]

--

.center[
.box-1.medium[Correlation vs. Causation]
]

---

class: text-slide, title-inv-7, center
count: FALSE

# Agenda 

--

- .left[Multivariate linear regression]
  - .left[Model evaluation]
  - .left[Omitted variable bias]
  - .left[Multicollinearity — correlated independent variable]

--

- .left[Hypothesis testing]
  - .left[Testing multiple parameters — T test vs. F test]

--

- .left[Variable transformations — interpreting results]
  - .left[Affine]
  - .left[Polynomial]
  - .left[Logarithmic]
  - .left[Dummy variables]

---

class: text-slide, main-slide, center, middle, hide-count

# Multivariate Regression

---

class: center

# Simple Regression

&lt;img src="img/simple-regression.jpg" width="75%"/&gt;

&lt;div class="my-footer"&gt;&lt;span&gt;https://www.statlearning.com&lt;/span&gt;&lt;/div&gt;

---

class: center

# Multivariate Regression

&lt;img src="img/multiple-regression.jpg" width="50%"/&gt;

&lt;div class="my-footer"&gt;&lt;span&gt;https://www.statlearning.com&lt;/span&gt;&lt;/div&gt;

---

class: text-slide

# Multivariate Regression

`\(y=\beta_1+\beta_1x_1+\beta_2x_2+\epsilon\)`
&lt;br&gt;&lt;br&gt;
How do we interpret `\(\beta_1\)`, `\(\beta_2\)`?
&lt;br&gt;
- `\(y=10+3x_1+4x_2\)`, `\(x_1=5\)`, `\(x_2=3\)` &lt;br&gt;&lt;br&gt;
- `\(y=10+18+20=48\)` &lt;br&gt;&lt;br&gt;
- 1 unit increase in `\(x_1\)` led to a `\(\beta_1\)` increase in `\(y\)` (just like bivariate regression)
- But what about `\(x_2\)`? It did not change. So this change is only true holding `\(x_2\)` constant
- We can hold `\(x_2\)` constant to see how `\(y\)` changes as `\(x_1\)` changes at that level of `\(x_2\)`

---

class: text-slide

# Evaluating the Model: Adjusted `\(\text{R}^2\)`

- Recall we can use `\(\text{R}^2=1-\text{SSR}/\text{TSS}\)`
- When we add a new independent variable, `\(\text{TSS}\)` does not change. `\(\text{TSS}=(u-\text{mean}(y))^2\)`
- However, the new variable will always cause `\(\text{SSR}\)`, `\((y-\hat{y})^2\)` to decrease. Therefore, `\(\text{R}^2\)` will always decrease, which makes adding more variables ostensibly better
- Adjusted `\(\text{R}^2\)` adds a disincentive (penalty) for adding new variables:
`$$\text{Adj R}^2=1-\frac{(n-1)}{n-k-1}\frac{\text{SSR}}{TSS}$$`
&lt;!--  Draw TSS vs. SSR --&gt;

---

class: text-slide, table-info

# Omitted variable bias

- If we do not use multiple regression, we may get biased estimate of the variable we do include
- "The bias results in the model attributing the effect of the missing variables to the estimated effects of the included variable."
- In other words, there are two variables that determine `\(y\)`, but our model only knows about one.
- The model we estimate with one variable accounts for the full effect of `\(y\)`, when we know the effect should be split between the two variables

---


class: text-slide, table-info

# Omitted variable bias 

- When will there be no omitted variable bias effect?
  1. The second variable has no effect on `\(y\)`. Therefore, there is no extra effect to go into the first variable
  2. `\(x_1\)` and `\(x_2\)` are completely unrelated. Even though `\(x_2\)` has an effect on `\(y\)`, `\(x_1\)` lacks that information

.left.column[
`$$\hat{\beta_1}=\frac{\hat{\text{Cov}}(X, Y)}{\hat{Var}(X)}$$`
]

$$
`\begin{split}
\hat{\text{Cov}}(\text{educ}, \text{wages}) &amp; = \hat{\text{Cov}}(\text{educ}, \beta_1\text{educ}+\beta_2\text{exp}+\epsilon) \\
 &amp; = \beta_1\hat{\text{Var}}(\text{educ}) + \beta_2\hat{\text{Cov}}(\text{educ}, \text{exp}) + \hat{\text{Cov}}(\text{educ}, \epsilon) \\
 &amp; = \beta_1\hat{\text{Var}}(\text{educ}) + \beta_2\hat{\text{Cov}}(\text{educ}, \text{exp})
\end{split}`
$$

$$
\text{Omitted variable bias: } \hat{\beta_1}=\beta_1+\beta_2\frac{\hat{\text{Cov}(\text{educ}, \text{exp})}}{\hat{\text{Var}}(\text{educ})}
$$

---

class: text-slide

# Calculating the bias effect

1. Population model (true relationship): `\(y=\beta_0+\beta_1x_1+\beta_2x_2+\nu\)`

2. Our model: `\(y=\hat{\beta}_0+\hat{\beta}_1x_1+\upsilon\)`

3. Auxiliary model: `\(x_2=\delta_0+\delta_1x_1+\epsilon\)`

.left-column[
- In the simple case of one regression and one omitted variable, estimating equation (2) by OLS will yield:

`$$\text{E}(\hat{\beta_1})=\beta_1+\beta_2\delta$$`
]

.right-column[
Equivalently, the bias is: `\(\text{E}(\hat{\beta_1})-\beta_1=\beta_2\delta\)`&lt;br&gt;&lt;br&gt;

|    |      A and B are&lt;br&gt;positively correlated      |  A and B are&lt;br&gt;negatively correlated |
|----------|:-------------:|:------:|
| B is positively&lt;br&gt;correlated with y |  Positive bias | Negative bias |
| B is negatively&lt;br&gt;correlated with y |    Negative bias   |   Positive bias |
]
---

class: text-slide

# Example: Bostom Housing Data

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; variable &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; description &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; CRIM &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; per capita crime rate by town &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; ZN &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; proportion of residential land zoned for lots over 25,000 sq.ft. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; INDUS &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; proportion of non-retail business acres per town. &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; CHAS &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Charles River dummy variable (1 if tract bounds river; 0 otherwise) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; NO &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; nitric oxides concentration (parts per 10 million) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; RM &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; average number of rooms per dwelling &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; AGE &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; proportion of owner-occupied units built prior to 1940 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; DIS &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; weighted distances to five Boston employment centres &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; RAD &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; index of accessibility to radial highways &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; TAX &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; full value property tax rate per $10,000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; PTRATIO &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; pupil teacher ratio by town &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; B &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1000(Bk 0.63)^2 where Bk is the proportion of blacks by town &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; LSTAT &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; % lower status of the population &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; MEDV &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Median value of owner-occupied homes in $1000's &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

class: text-slide, center

&lt;img src="img/correlation_bostom_matrix.png" width="80%"/&gt;

---

class: text-slide, table-info

# 2,000 Regressions

- Take a random sample of 90% people out of the 506 that are in the Boston Housing data set
- Our model will be `\(y=\beta_1x_1+\beta_2x_2+\epsilon\)`, where `\(\beta_1=\text{age}\)` and `\(\beta_2=\text{rm}\)`
- Estimate `\(\beta_1\)` using OLS (NOT controlling for `\(\text{rm}\)`) with the sample
- Estimate `\(\beta_1\)` using OLS, controlling for `\(\text{rm}\)` with the same sample
- Repeat 2,000 times

.left-column[### Our data:]

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; crim &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; zn &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; indus &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; chas &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; nox &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; rm &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; age &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; dis &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; rad &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; tax &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; ptratio &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; b &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; lstat &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; medv &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0.00632 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 18 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.31 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.538 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6.575 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 65.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.0900 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 296 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 15.3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 396.90 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.98 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 24.0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0.02731 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7.07 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.469 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6.421 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 78.9 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.9671 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 242 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 17.8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 396.90 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 9.14 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 21.6 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0.02729 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7.07 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.469 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7.185 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 61.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.9671 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 242 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 17.8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 392.83 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.03 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 34.7 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0.03237 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.18 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.458 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6.998 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 45.8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6.0622 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 222 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 18.7 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 394.63 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.94 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 33.4 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0.06905 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.18 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.458 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7.147 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 54.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6.0622 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 222 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 18.7 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 396.90 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; NA &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 36.2 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0.02985 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.18 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.458 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6.430 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 58.7 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 6.0622 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 222 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 18.7 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 394.12 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.21 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 28.7 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

class: text-slide, center

# `\(\beta_1\)` is underestimated when `\(\beta_2\)` is ommitted

&lt;img src="img/ommitted_variable.png" width="78%"/&gt;

---

class: text-slide, main-slide, center, middle, hide-count

# Multicollinearity

---

class: text-slide

# Multicollinearity

- Multivariate linear models cannot handle perfect multicollinearity
- Example: we have two variables: `\(x_1\)` and `\(x_2=3 \times x_1\)`
- Fit model to predict `\(y\)` with `\(x_1\)` and `\(x_2\)`:
  - `\(y=\beta_0+\beta_1x_1+\text{NA}\)`, where `\(\text{NA}\)` stands for not a value &lt;br&gt;&lt;br&gt;
- We can think of this as `\(\beta_1\)` containing the entire effect for both `\(x_1\)` and `\(x_2\)`. After all, these variables are the same.
- Including highly correlated variables in our model will not produce biased estimates, but it will harm our precision.

---

class: text-slide

# Baseball example

- Use home runs, batting average, and RBI to predict salary
- Variables are defined as follows:
  - `\(\text{salary}=\text{homeruns}\times 10,000+\epsilon\)`
  - `\(\text{BA}=\text{homeruns}+270+\epsilon\)`
  - `\(\text{RBI}=\text{homeruns}\times 3+\epsilon\)`
  - Example: `\(\text{homeruns}=30\)`, `\(\text{BA}=300\)`, `\(\text{RBI}=90\)`, `\(\text{salary}=300,000\)`
- Fit a model for each variable individually:
  - `\(\text{salary}=9,934.27\times\text{HR}\)`
  - `\(\text{salary}=1,002.95\times\text{BA}\)`
  - `\(\text{salary}=3,291.02\times\text{RBI}\)`
- Fit a model with all three: `\(\text{salary}=9,226.169\times \text{HR}+225.884 \times \text{RBI}+2.982 \times\text{BA}\)`
- What is this model saying? Why not: `\(\text{salary}=9,934.27\times \text{HR}+3,291.02 \times \text{RBI}+1,002.95 \times\text{BA}\)`

---

class: text-slide

# Helpful resource

- Omitted variable bias and multicollinearity discussion: https://are.berkeley.edu/courses/EEP118/current/handouts/OVB%20versus%20Multicollinearity_eep118_sp15.pdf


|Situation|Action|
|:----------|:-------------|
| z is correlated with both x and y | Probably best to include z but be wary of multicollinearity
| z is correlated with x but not y | Do not include z — no benefit
| z is correlated with y but not x | Include z — new explanatory power
| z is correlated with neither x nor y | Should not be much effect when including,&lt;br&gt;but could affect hypothesis testing — no real benefit

---

class: text-slide, main-slide, center, middle, hide-count

# Hypothesis testing

---

class: text-slide

# Hypothesis testing

.pull-left[
- The previous example demonstrates why we must use F test to test all hypothesis simultaneously rather than a T test
- Recall the T test for `\(H_0 \rightarrow \hat{\beta}_1=\theta\text{:} \frac{(\hat{\beta}_1-\theta)}{\text{SE}(\hat{\beta}_1)}\)`
- The above statistic is t-distributed under the null hypothesis, so we can see how likely it would be to get the above value from a t distribution
- If we are testing multiple hypotheses, we can apply the same logic as long as we know how that statistic is distributed. In this new test, our statistic belongs to the F distribution
]

.pull-right[
&lt;img src="index_files/figure-html/unnamed-chunk-3-1.png" width="2100" /&gt;
]

---

class: text-slide

# Back to baseball

- To perform an F test, we compare a model with restrictions to a model without restrictions and see if there is a significant difference. Think of restrictions as features not included in the model
- `\(\text{salary}=\text{years}+\text{gmsYear}+\text{HR}+\text{RBI}+\text{BA}\)`
- If `\(\text{HR}\)`, `\(\text{RBI}\)`, `\(\text{BA}\)` all have no effect on `\(\text{salary}\)`, then the model `\(\text{salary}=\text{years}+\text{gmsYears}\)` should perform just as well
- How do we measure *performance*? Sum of squared residuals (SSR)!
- Test statistics: `\(\frac{\text{SSR}_\text{r}-\text{SSR}_\text{ur}/q}{\text{SSR}_\text{ur}/(n-k-1)}\)`
- The above fraction is the ratio of two chi squared variables divided by their degrees of freedom, which makes this F-distributed
- Remember adding variables can only improve the model, so the F statistic will always be positive

---

class: text-slide, main-slide, center, middle, hide-count

# Types of variables and transformations

---

class: text-slide

# Affine

- Affine transformations are transformations that do not affect the fit of the model. The most common example is scaling transformations

- Example:
  - `\(\text{weight(lbs)}=5+2.4\times \text{height(inches)}\)`
  - `\(\text{weight(lbs)}=5+0.094\times \text{height(mm)}\)`

- This is why scaling variables is not necessary for linear regression, but knowing the scale of your variables is important for interpretation

---

class: text-slide

# Polynomial

- Linear regression can still be used to fit data with a non-linear distribution
- The model is linear in parameters, not necessarily variables
- i.e. we must have `\(\beta_1\)`, `\(\beta_2\)`, `\(\beta_3\)`, but we can utilize `\(x_1^2\)` or `\(x_2/x_3\)`
- We might leverage the above to generate a curved regression line, providing a better fit in some cases
- How do we now interpret the coefficients?

`$$\hat{\text{wage}}=3.12+.447\text{exp}-0.007\text{exp}^2$$`

- The big difference is the effect of an increase in experience on wage now depends on the level of experience

---

class: text-slide

# Logarithmic

Recall that the natural logarithm is the inverse of the exponential function, so `\(\text{ln}(e^x)=x\)`, and:
&lt;br&gt;&lt;br&gt;
.pull-left[
`\(\text{ln}(1)=0\)`&lt;br&gt;&lt;br&gt;
`\(\text{ln}(0)=-\infty\)`&lt;br&gt;&lt;br&gt;
`\(\text{ln}(ax)=\text{ln}(a)+\text{ln}(x)\)`&lt;br&gt;&lt;br&gt;
]
.pull-right[
`\(\text{ln}(x^a)=a\text{ln}(x)\)`&lt;br&gt;&lt;br&gt;
`\(\text{ln}(\frac{1}{x})=-\text{ln}(x)\)`&lt;br&gt;&lt;br&gt;
`\(\text{ln}(\frac{x}{a})=\text{ln}(x) - \text{ln}(a)\)`&lt;br&gt;&lt;br&gt;
`\(\frac{d\text{ln}(x)}{dx}=\frac{1}{x}\)`
]

---

class: text-slide

# Interpreting log variables

- `\(\beta_0=5\)`, `\(\beta_1=0.2\)`
- Level-log: `\(y=5+0.2\text{ln}(x)\)`
  - 1% change in `\(x=\beta_1/100\)` change in `\(y\)`
&lt;br&gt;&lt;br&gt;

- Log-level: `\(\text{ln}(y)=5+0.2(x)\)`
  - 1 unit change in `\(x=\beta_1 \times 100\text{%}\)` change in `\(y\)`
&lt;br&gt;&lt;br&gt;

- Log-log: `\(\text{ln}(y)=5+0.2\text{ln}(x)\)`
  - 1% change in `\(x=\beta_1\text{%}\)` change in `\(y\)`
&lt;br&gt;&lt;br&gt;

---

class: text-slide

# Dummy variables

- Dummy variables is how categorical variables can be mathematically represented
- They represent groups or place continuous variables into bins
- What is this regression telling us?
  - `\(\text{nbaSalary}=5\times \text{PPG}+10.5\times \text{guard}+9.6\times \text{forward}+10.8\times \text{center}\)`
&lt;br&gt;&lt;br&gt;
- Do we need dummy variables for `\(guard\)`, `\(forward\)`, `\(center\)`?
- How would the regression change if we only used 2 out of 3?
- `\(\text{nbaSalary}=10.5 + 5\times \text{PPG}-0.9\times \text{forward}+0.3\times \text{center}\)`
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": true,
"slideNumberFormat": "%current%",
"ratio": "16:9",
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
